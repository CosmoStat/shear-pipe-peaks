{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sp_peaks\n",
    "from sp_peaks import slics\n",
    "from sp_peaks import mapping\n",
    "from sp_peaks import summary_statistics\n",
    "from sp_peaks import plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from lenspack.geometry.projections.gnom import radec2xy\n",
    "from lenspack.utils import bin2d\n",
    "from lenspack.image.inversion import ks93\n",
    "import lenspack.peaks as peaks\n",
    "from lenspack.starlet_l1norm import noise_coeff, get_l1norm_noisy\n",
    "from lenspack.image.transforms import starlet2d\n",
    "from astropy.stats import mad_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make master file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory\n",
    "root_directory = \"/n17data/tersenov/Cov_DES_SLICS\"\n",
    "\n",
    "# Open the master text file in write mode\n",
    "master_file_path = \".././input/master_file_cov.txt\"\n",
    "with open(master_file_path, \"w\") as master_file:\n",
    "    for subdir in os.listdir(root_directory):\n",
    "        # Check if the subdirectory name matches the pattern \"LOS...\"\n",
    "        if subdir.startswith(\"LOS\") and subdir[3:].isdigit():\n",
    "            subdir_path = os.path.join(root_directory, subdir)\n",
    "            \n",
    "            # Iterate over the files in the subdirectory\n",
    "            for file_name in os.listdir(subdir_path):\n",
    "                # Check if the file name matches the desired pattern\n",
    "                if file_name.startswith(\"DES_MocksCat_SLICS_4_Bin\") and file_name.endswith(\".dat\"):\n",
    "                    file_path = os.path.join(subdir_path, file_name)\n",
    "                    master_file.write(file_path + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the \"master_file_cov.txt\"\n",
    "master_file_path = \".././input/master_file_cov.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file paths from the \"master_file_cov.txt\"\n",
    "with open(master_file_path, \"r\") as file:\n",
    "    file_paths = file.readlines()\n",
    "    file_paths = [path.strip() for path in file_paths]\n",
    "\n",
    "# Parse these file paths\n",
    "parsed_cov_data = slics.parse_cov_SLICS_filenames(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct 124 realisations of the survey by picking each tile from a random LOS, ensuring that each file is only included once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "los_numbers = np.unique(parsed_cov_data['LOS']) # List of all LOS numbers\n",
    "num_realizations = 124 # Number of realizations\n",
    "num_tiles_per_realization = 19 # Number of tiles to select for each realization\n",
    "\n",
    "num_bins = 4\n",
    "bin_number = 2\n",
    "\n",
    "# Reconstruct 124 realisations of the survey by picking each tile from a random LOS, ensuring that each file is only included once.\n",
    "collections_of_files = slics.survey_realizations_reconstruction(num_realizations, num_tiles_per_realization, bin_number, parsed_cov_data['LOS'], file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak counts datavector calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Parameters\n",
    "N_GAL = 7\n",
    "PIX_ARCMIN = 0.4\n",
    "SHAPE_NOISE = 0.44\n",
    "NSCALES = 5\n",
    "NBINS = 40 \n",
    "KAPPA_SNR = np.linspace(-2, 6, 31)\n",
    "NUM_REALIZATIONS = 124  # Number of realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store data vectors for each realization\n",
    "data_vectors = []\n",
    "\n",
    "# Loop over realizations\n",
    "for realization_files in collections_of_files[:2]: # Use only the first ... realizations for now\n",
    "    # Initialize an empty list to store peak counts vectors for each tile in this realization\n",
    "    peak_counts_realization = []\n",
    "\n",
    "    # Loop over files (tiles) in this realization\n",
    "    for tile_file in realization_files:\n",
    "        # Load the catalog data for this tile\n",
    "        catalog_data = slics.read_catalogue_pd(tile_file)\n",
    "\n",
    "        # Extract data from the catalog\n",
    "        ra = catalog_data['RA']\n",
    "        dec = catalog_data['Dec']\n",
    "        g1_sim = catalog_data['gamma1_sim']\n",
    "        g2_sim = catalog_data['gamma2_sim']\n",
    "\n",
    "        # Calculate peak counts for this tile\n",
    "        e1map, e2map = mapping.bin_shear_field(ra, dec, g1_sim, g2_sim)\n",
    "        kappaE, _ = ks93(e1map, -e2map)\n",
    "        kappaE_noisy, noise_map_CFIS_z05 = mapping.add_noise_to_kappa_map(kappaE, SHAPE_NOISE, N_GAL, PIX_ARCMIN)\n",
    "        kappaE_noisy_smoothed = mapping.smooth_kappa_map(kappaE_noisy, PIX_ARCMIN) \n",
    "        snr = mapping.convert_to_snr_map(kappaE_noisy_smoothed, kappaE_noisy_smoothed)\n",
    "        kappa_th_center_snr, peak_counts_single = summary_statistics.compute_single_scale_peak_counts(snr, KAPPA_SNR)\n",
    "\n",
    "        # Append peak counts for this tile to the list\n",
    "        peak_counts_realization.append(peak_counts_single)\n",
    "\n",
    "    # Compute the average peak counts vector for this realization\n",
    "    average_peak_counts = np.mean(peak_counts_realization, axis=0)\n",
    "\n",
    "    # Append the average peak counts vector for this realization to the list of data vectors\n",
    "    data_vectors.append(average_peak_counts)\n",
    "\n",
    "# Convert the list of data vectors into a NumPy array\n",
    "data_vectors = np.array(data_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak counts covariance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average peak counts vector across all realizations\n",
    "mean_PC_over_realizations = np.mean(data_vectors, axis=0)\n",
    "\n",
    "# Compute the deviations of peak counts in each realization from the average vector\n",
    "deviations = data_vectors - mean_PC_over_realizations\n",
    "\n",
    "# Compute the covariance matrix\n",
    "# The covariance_matrix will be a square matrix of shape (num_bins, num_bins).\n",
    "num_realizations = data_vectors.shape[0]\n",
    "covariance_matrix = np.dot(deviations.T, deviations) / (num_realizations - 1)\n",
    "\n",
    "# # Normalize the covariance matrix to the unity of the diagonal\n",
    "# diagonal_sqrt = np.sqrt(np.diag(covariance_matrix))\n",
    "# covariance_matrix_normalized = covariance_matrix / np.outer(diagonal_sqrt, diagonal_sqrt)\n",
    "\n",
    "# Normalize the covariance matrix\n",
    "max_cov_value = np.max(covariance_matrix)\n",
    "covariance_matrix_normalized = covariance_matrix / max_cov_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_map(covariance_matrix_normalized, title='Covariance Matrix of Peak Counts', cmap='viridis', vmin=None, vmax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiscale peak counts datavector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Parameters\n",
    "N_GAL = 7\n",
    "PIX_ARCMIN = 0.4\n",
    "SHAPE_NOISE = 0.44\n",
    "NSCALES = 5\n",
    "NBINS = 40\n",
    "KAPPA_SNR = np.linspace(-2, 6, 31)\n",
    "NUM_REALIZATIONS = 124  # Number of realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store data the Multi-Scale Peak Counts vectors for each realization\n",
    "MS_PC_data_vectors = []\n",
    "\n",
    "# Loop over realizations\n",
    "for realization_files in collections_of_files[:10]:  # Use only the first ... realizations for now\n",
    "    # Initialize an empty list to store peak counts vectors for each scale in this realization\n",
    "    peak_counts_realization = []\n",
    "\n",
    "    # Loop over files (tiles) in this realization\n",
    "    for tile_file in realization_files:\n",
    "        # Load the catalog data for this tile\n",
    "        catalog_data = slics.read_catalogue_pd(tile_file)\n",
    "\n",
    "        # Extract data from the catalog\n",
    "        ra = catalog_data['RA']\n",
    "        dec = catalog_data['Dec']\n",
    "        g1_sim = catalog_data['gamma1_sim']\n",
    "        g2_sim = catalog_data['gamma2_sim']\n",
    "\n",
    "        # Calculate shear and kappa maps\n",
    "        e1map, e2map = mapping.bin_shear_field(ra, dec, g1_sim, g2_sim)\n",
    "        kappaE, _ = ks93(e1map, -e2map)\n",
    "\n",
    "        # Add noise to the kappa map\n",
    "        kappaE_noisy, noise_map_CFIS_z05 = mapping.add_noise_to_kappa_map(kappaE, SHAPE_NOISE, N_GAL, PIX_ARCMIN)\n",
    "\n",
    "        # Smooth the noisy kappa map\n",
    "        kappaE_noisy_smoothed = mapping.smooth_kappa_map(kappaE_noisy, PIX_ARCMIN)\n",
    "\n",
    "        # Compute SNR map\n",
    "        snr = mapping.convert_to_snr_map(kappaE_noisy_smoothed, kappaE_noisy_smoothed)\n",
    "\n",
    "        # Compute multiscale SNR maps\n",
    "        multiscale_snr_maps = mapping.compute_multiscale_snr_maps(kappaE, noise_map_CFIS_z05, NSCALES)\n",
    "\n",
    "        # Compute peak counts for each scale\n",
    "        kappa_th_center_snr, peak_counts_multi = summary_statistics.compute_multiscale_peak_counts(multiscale_snr_maps, KAPPA_SNR)\n",
    "\n",
    "        # Append peak counts for this tile to the list for this realization\n",
    "        peak_counts_realization.append(peak_counts_multi)\n",
    "\n",
    "    # Compute the average peak counts vector for this realization\n",
    "    average_peak_counts = np.mean(peak_counts_realization, axis=0)\n",
    "\n",
    "    # Append the average peak counts vector for this realization to the list of data vectors\n",
    "    MS_PC_data_vectors.append(average_peak_counts)\n",
    "\n",
    "# Convert the list of data vectors into a NumPy array\n",
    "MS_PC_data_vectors = np.array(MS_PC_data_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data vectors to a file\n",
    "np.save('.././output/MS_PC_data_vectors.npy', MS_PC_data_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data vectors from a file\n",
    "MS_PC_data_vectors = np.load('.././output/MS_PC_data_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectors_reshaped = MS_PC_data_vectors.reshape(10, -1)  \n",
    "\n",
    "# Compute the average peak counts vector across all realizations for multiscale peak counts\n",
    "mean_PC_over_realizations_multi = np.mean(data_vectors_reshaped, axis=0)\n",
    "\n",
    "# Compute the deviations of peak counts in each realization from the average vector\n",
    "deviations_multi = data_vectors_reshaped - mean_PC_over_realizations_multi\n",
    "\n",
    "# Compute the covariance matrix for multiscale peak counts\n",
    "num_realizations_multi, num_bins_multi = data_vectors_reshaped.shape\n",
    "covariance_matrix_multi = np.dot(deviations_multi.T, deviations_multi) / (num_realizations_multi - 1)\n",
    "\n",
    "# Normalize the covariance matrix for multiscale peak counts\n",
    "diagonal_multi = np.sqrt(np.diag(covariance_matrix_multi))\n",
    "# Add a small epsilon to the diagonal to avoid division by zero\n",
    "epsilon = 1e-10\n",
    "covariance_matrix_normalized_multi = covariance_matrix_multi / (np.outer(diagonal_multi, diagonal_multi)+epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_map(covariance_matrix_multi, title='Covariance Matrix of Peak Counts', cmap='viridis', vmin=None, vmax=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_map(covariance_matrix_normalized_multi, title='Covariance Matrix of Peak Counts', cmap='viridis', vmin=None, vmax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l1-norm datavector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store data vectors for the L1-norm histogram for each realization\n",
    "L1_norm_data_vectors = []\n",
    "\n",
    "# Loop over realizations\n",
    "for realization_files in collections_of_files[:10]:  # Use only the first ... realizations for now\n",
    "    # Initialize an empty list to store L1-norm histogram vectors for each scale in this realization\n",
    "    l1_norm_histogram_realization = []\n",
    "\n",
    "    # Loop over files (tiles) in this realization\n",
    "    for tile_file in realization_files:\n",
    "        # Load the catalog data for this tile\n",
    "        catalog_data = slics.read_catalogue_pd(tile_file)\n",
    "\n",
    "        # Extract data from the catalog\n",
    "        ra = catalog_data['RA']\n",
    "        dec = catalog_data['Dec']\n",
    "        g1_sim = catalog_data['gamma1_sim']\n",
    "        g2_sim = catalog_data['gamma2_sim']\n",
    "\n",
    "        # Calculate shear and kappa maps\n",
    "        e1map, e2map = mapping.bin_shear_field(ra, dec, g1_sim, g2_sim)\n",
    "        kappaE, _ = ks93(e1map, -e2map)\n",
    "\n",
    "        # Add noise to the kappa map\n",
    "        kappaE_noisy, noise_map_CFIS_z05 = mapping.add_noise_to_kappa_map(kappaE, SHAPE_NOISE, N_GAL, PIX_ARCMIN)\n",
    "\n",
    "        # Compute the L1-norm histogram for each scale\n",
    "        bins_l1, l1norm_histogram = get_l1norm_noisy(kappaE, noise_map_CFIS_z05, NSCALES, NBINS*2)\n",
    "\n",
    "        # Append the L1-norm histogram for this tile to the list for this realization\n",
    "        l1_norm_histogram_realization.append(l1norm_histogram)\n",
    "\n",
    "    # Compute the average L1-norm histogram vector for this realization\n",
    "    average_l1_norm_histogram = np.mean(l1_norm_histogram_realization, axis=0)\n",
    "\n",
    "    # Append the average L1-norm histogram vector for this realization to the list of data vectors\n",
    "    L1_norm_data_vectors.append(average_l1_norm_histogram)\n",
    "\n",
    "# Convert the list of data vectors into a NumPy array\n",
    "L1_norm_data_vectors = np.array(L1_norm_data_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data vectors to a file\n",
    "np.save('.././output/L1_norm_data_vectors.npy', L1_norm_data_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data vectors from a file\n",
    "L1_norm_data_vectors = np.load('.././output/L1_norm_data_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_norm_data_vectors_reshaped = L1_norm_data_vectors.reshape(10, -1)\n",
    "\n",
    "# Compute the average L1-norm histogram vector across all realizations for the L1-norm\n",
    "mean_L1_norm_over_realizations = np.mean(L1_norm_data_vectors_reshaped, axis=0)\n",
    "\n",
    "# Compute the deviations of L1-norm histograms in each realization from the average vector\n",
    "deviations_L1_norm = L1_norm_data_vectors_reshaped - mean_L1_norm_over_realizations\n",
    "\n",
    "# Compute the covariance matrix for the L1-norm histograms\n",
    "num_realizations_L1_norm, num_bins_L1_norm = L1_norm_data_vectors_reshaped.shape\n",
    "covariance_matrix_L1_norm = np.dot(deviations_L1_norm.T, deviations_L1_norm) / (num_realizations_L1_norm - 1)\n",
    "\n",
    "# Calculate the diagonal of the covariance matrix\n",
    "diagonal_L1_norm = np.sqrt(np.diag(covariance_matrix_L1_norm))\n",
    "\n",
    "# Calculate the correlation coefficients\n",
    "correlation_matrix_L1_norm = covariance_matrix_L1_norm / np.outer(diagonal_L1_norm, diagonal_L1_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_map(covariance_matrix_L1_norm, title='Covariance Matrix of L1-norm Histograms', cmap='viridis', vmin=None, vmax=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_map(correlation_matrix_L1_norm, title='Correlation Matrix of L1-norm Histograms', cmap='viridis', vmin=None, vmax=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
