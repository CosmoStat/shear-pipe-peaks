{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define root directory where the simulation files are located\n",
    "root_directory = \"/n17data/tersenov/SLICS/Cosmo_DES\"\n",
    "\n",
    "# Directories to exclude\n",
    "exclude_dirs = [\"SLICS_HR\", \"fid_a\", \"fid_f\"]\n",
    "\n",
    "# Open the master text file in write mode\n",
    "master_file_path = \"master_file.txt\"\n",
    "with open(master_file_path, \"w\") as master_file:\n",
    "    # Iterate over the files and subdirectories in the root directory\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        # Exclude the specified directories\n",
    "        dirs[:] = [d for d in dirs if d not in exclude_dirs]\n",
    "        \n",
    "        for file_name in files:\n",
    "            # Write the file path to the master file\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            master_file.write(file_path + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# this should go in SLICS interface module- check martin's existing part of SLICS interface module, this may be carried out in a simplier way\n",
    "\n",
    "def read_SLICS_cats(file_paths):\n",
    "    \"\"\"Reads the information in the filename.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_paths : list of str (.txt file)\n",
    "        List containing the paths to the files to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : recarray\n",
    "        Numpy recarray containing the information extracted from the file names\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Make empty recarray to store the data\n",
    "    data = np.recarray(len(file_paths), dtype=[('id', int), ('seed', 'U1'), ('bin', int), ('LOS', int), ('tile', int)])\n",
    "\n",
    "    # Iterate over the file paths and process each file\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        # Extract the file name from the file path\n",
    "        file_name = file_path.split(\"/\")[-1]\n",
    "        \n",
    "        # Split file name into parts\n",
    "        file_parts = file_name.split(\"_\")\n",
    "        \n",
    "        id = int(file_parts[2])\n",
    "        seed = file_parts[3]\n",
    "        unknown_number = int(file_parts[4])\n",
    "        bin = int(file_parts[5][3:])  # Extract the number after \"Bin\"\n",
    "        LOS = int(file_parts[6][3:])  # Extract the number after \"LOS\"\n",
    "        tile = int(file_parts[7][1:-4])  # Extract the number after \"R\"\n",
    "\n",
    "        # Assign the extracted data to the corresponding fields in the recarray\n",
    "        data[i]['id'] = id\n",
    "        data[i]['seed'] = seed\n",
    "        data[i]['bin'] = bin\n",
    "        data[i]['LOS'] = LOS\n",
    "        data[i]['tile'] = tile\n",
    "        # print(data[i])\n",
    "\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file paths from master_file.txt\n",
    "filename = \"master_file.txt\"\n",
    "with open(filename, 'r') as file:\n",
    "    file_paths = file.readlines()\n",
    "    file_paths = [path.strip() for path in file_paths]\n",
    "\n",
    "data = read_SLICS_cats(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should go in SLICS interface module\n",
    "def read_SLICS_cosmo_params(file_path):\n",
    "    \"\"\"Reads the cosmological parameters from the .dat file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the .dat file containing the cosmological parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cosmo_params : dict\n",
    "        Dictionary mapping each ID to its corresponding cosmological parameters\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cosmo_params = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        # Iterate over the lines starting from the second line\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                id = int(parts[0])\n",
    "                params = {\n",
    "                    'Om': float(parts[1]),\n",
    "                    'h': float(parts[2]),\n",
    "                    'w_0': float(parts[3]),\n",
    "                    'sigma_8': float(parts[4]),\n",
    "                    'Oc': float(parts[5])\n",
    "                }\n",
    "                cosmo_params[id] = params\n",
    "\n",
    "    return cosmo_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be a function\n",
    "\n",
    "# Path to the .dat file\n",
    "dat_file_path = \"/home/tersenov/shear-pipe-peaks/example/CosmoTable.dat\"\n",
    "\n",
    "# Read the cosmological parameters from the .dat file\n",
    "cosmo_params = read_SLICS_cosmo_params(dat_file_path)\n",
    "\n",
    "# Map the IDs in the recarray to the corresponding cosmological parameters\n",
    "mapped_params = []\n",
    "for row in data:\n",
    "    id = row['id']\n",
    "    params = cosmo_params.get(id)\n",
    "    if params:\n",
    "        mapped_params.append(params)\n",
    "    else:\n",
    "        print(f\"No parameters found for ID {id}\")\n",
    "\n",
    "# Now, 'mapped_params' will contain the corresponding cosmological parameters for each ID in the recarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the mass map from the catalog, make the S/N maps and count the peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catalog columns:\n",
    "\n",
    "0. RA\n",
    "1. DEC\n",
    "2. e1_data\n",
    "3. e2_data\n",
    "4. w \n",
    "5. redshift_true_sim\n",
    "6. gamma1_sim\n",
    "7. gamma2_sim\n",
    "8. kappa_sim\n",
    "9. s_metacal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from lenspack.geometry.projections.gnom import radec2xy\n",
    "from lenspack.utils import bin2d\n",
    "from lenspack.image.inversion import ks93\n",
    "import lenspack.peaks as peaks\n",
    "from lenspack.starlet_l1norm import noise_coeff, get_l1norm_noisy\n",
    "from lenspack.image.transforms import starlet2d\n",
    "from astropy.stats import mad_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Parameters\n",
    "CATALOG_FILE = \"/n17data/tersenov/SLICS/Cosmo_DES/16_a/LOS4/DES_MocksCat_16_a_4_Bin3_LOS4_R4.dat\"\n",
    "N_GAL = 7\n",
    "PIX_ARCMIN = 0.4\n",
    "SHAPE_NOISE = 0.44\n",
    "NSCALES = 5\n",
    "NBINS = 40 \n",
    "KAPPA_SNR = np.linspace(-2, 6, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I feel like we already have this function with the SLICS interface\n",
    "def read_catalog_data(catalog_data):\n",
    "    ra = catalog_data[0]\n",
    "    dec = catalog_data[1]\n",
    "    g1_sim = catalog_data[6]\n",
    "    g2_sim = catalog_data[7]\n",
    "    kappa_sim = catalog_data[8]\n",
    "    return ra, dec, g1_sim, g2_sim, kappa_sim\n",
    "\n",
    "# we should discuss this- the following functions will go into a mass-map module that will \n",
    "# contain the different methods.  do they all require binned data?, \n",
    "# at any rate, we should have a separate binning function\n",
    "\n",
    "def create_kappa_map(ra, dec, g1_sim, g2_sim, size_x_deg=10, size_y_deg=10, pixel_size_emap_amin=0.4):\n",
    "    x, y = radec2xy(np.mean(ra), np.mean(dec), ra, dec) # Project (ra,dec) -> (x,y)\n",
    "\n",
    "    Nx = int(size_x_deg / pixel_size_emap_amin * 60)\n",
    "    Ny = int(size_y_deg / pixel_size_emap_amin * 60)\n",
    "\n",
    "    e1map, e2map = bin2d(x, y, npix=(Nx, Ny), v=(g1_sim, g2_sim)) # bin the shear field into a 2D map\n",
    "    emap = np.array([e1map,e2map]) # stack the two components into a single array\n",
    "\n",
    "    kappaE, kappaB = ks93(e1map, -e2map) # make kappa map (the minus sign has to be here for our data conventions)\n",
    "    return kappaE, kappaB\n",
    "\n",
    "\n",
    "def add_noise_to_kappa_map(kappa_map, shape_noise, n_gal, pix_arcmin):\n",
    "    sigma_noise_CFIS = shape_noise / (np.sqrt(2 * n_gal * pix_arcmin**2))\n",
    "    noise_map_CFIS_z05 = sigma_noise_CFIS * np.random.randn(kappa_map.shape[0], kappa_map.shape[1]) # generate noise map\n",
    "    kappa_map_noisy = kappa_map + noise_map_CFIS_z05 # Add noise to the mass map\n",
    "    return kappa_map_noisy, noise_map_CFIS_z05\n",
    "\n",
    "def smooth_kappa_map(kappa_map, pixel_size_emap_amin):\n",
    "    # Set the standard deviation of the Gaussian filter based on the pixel size of the kappa map\n",
    "    precision_Peaks = 2 / pixel_size_emap_amin # pixel_size_emap_amin is the pixel size of the kappa map in arcminutes\n",
    "    kappa_map_smoothed = ndi.gaussian_filter(kappa_map, precision_Peaks)\n",
    "    return kappa_map_smoothed\n",
    "\n",
    "def convert_to_snr_map(kappa_map_smoothed, noise_map_smoothed):\n",
    "    snr_map = kappa_map_smoothed / np.std(noise_map_smoothed)\n",
    "    return snr_map\n",
    "\n",
    "def compute_multiscale_snr_maps(image, noise, nscales):\n",
    "    \"\"\"\n",
    "    Compute SNR maps for each wavelet scale of a noisy image.\n",
    "    \n",
    "    Parameters:\n",
    "        image (numpy.ndarray): The noiseless image.\n",
    "        noise (numpy.ndarray): The noise to be added to the image.\n",
    "        nscales (int): Number of wavelet scales for starlet decomposition.\n",
    "        \n",
    "    Returns:\n",
    "        snr_maps (list of numpy.ndarray): List of SNR maps for each scale.\n",
    "    \"\"\"\n",
    "    # Add noise to the noiseless image\n",
    "    image_noisy = image + noise\n",
    "    \n",
    "    # Perform starlet decomposition\n",
    "    image_starlet = starlet2d(image_noisy, nscales)\n",
    "    \n",
    "    # Estimate the noise level\n",
    "    noise_estimate = mad_std(image_noisy)\n",
    "    coeff_j = noise_coeff(image, nscales)\n",
    "    \n",
    "    snr_maps = []\n",
    "    for image_j, std_co in zip(image_starlet, coeff_j):\n",
    "        sigma_j = std_co * noise_estimate\n",
    "        \n",
    "        # Compute SNR map\n",
    "        snr_map = image_j / sigma_j\n",
    "        snr_maps.append(snr_map)\n",
    "    \n",
    "    return snr_maps\n",
    "# this should go into the summary statistics module that will have single scale peak counts, multi-scale peak counts, l1 norm, etc\n",
    "def compute_single_scale_peak_counts(snr_map, kappa_snr):\n",
    "    \"\"\"\n",
    "    Compute peak counts for a single SNR map.\n",
    "\n",
    "    Parameters:\n",
    "        snr_map (numpy.ndarray): SNR map.\n",
    "        kappa_snr (numpy.ndarray): Array of kappa values corresponding to the SNR map.\n",
    "\n",
    "    Returns:\n",
    "        kappa_th_center_snr (numpy.ndarray): Array of kappa threshold centers for peak counts.\n",
    "        peak_counts (numpy.ndarray): Peak counts for the given SNR map.\n",
    "    \"\"\"\n",
    "    kappa_th_center_snr = 0.5 * (kappa_snr[:-1] + kappa_snr[1:])\n",
    "    peak_counts = peaks.peaks_histogram(snr_map, kappa_snr)[0]\n",
    "    return kappa_th_center_snr, peak_counts\n",
    "\n",
    "\n",
    "def compute_multiscale_peak_counts(snr_maps, kappa_snr):\n",
    "    \"\"\"\n",
    "    Compute peak counts for each wavelet scale of SNR maps.\n",
    "\n",
    "    Parameters:\n",
    "        snr_maps (list of numpy.ndarray): List of SNR maps for each scale.\n",
    "        kappa_snr (numpy.ndarray): Array of kappa values corresponding to SNR maps.\n",
    "\n",
    "    Returns:\n",
    "        kappa_th_center_snr (numpy.ndarray): Array of kappa threshold centers for peak counts.\n",
    "        peak_counts (list of numpy.ndarray): List of peak counts for each scale.\n",
    "    \"\"\"\n",
    "    kappa_th_center_snr = 0.5 * (kappa_snr[:-1] + kappa_snr[1:])\n",
    "    \n",
    "    peak_counts = [peaks.peaks_histogram(snr_map, kappa_snr)[0] for snr_map in snr_maps]\n",
    "\n",
    "    return kappa_th_center_snr, peak_counts\n",
    "\n",
    "def plot_map(map_data, title='', cmap='inferno', vmin=None, vmax=None):\n",
    "    \"\"\"\n",
    "    Plot a 2D map using a colormap.\n",
    "\n",
    "    Parameters:\n",
    "        map_data (numpy.ndarray): The 2D map data.\n",
    "        title (str): Title of the plot.\n",
    "        cmap (str): Colormap name.\n",
    "        vmin (float): Minimum value for colormap scaling.\n",
    "        vmax (float): Maximum value for colormap scaling.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    img = plt.imshow(map_data, cmap=cmap, vmin=vmin, vmax=vmax, origin='lower')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(img)\n",
    "    plt.show()    \n",
    "\n",
    "def plot_peak_count_histograms(kappa_th_center, peak_counts, title, xlabel, ylabel, log_scale=False):\n",
    "    \"\"\"\n",
    "    Plot histograms of peak counts.\n",
    "\n",
    "    Parameters:\n",
    "        kappa_th_center (numpy.ndarray): Array of kappa threshold centers.\n",
    "        peak_counts (numpy.ndarray or list of numpy.ndarray): Peak counts for each scale.\n",
    "        title (str): Title of the plot.\n",
    "        xlabel (str): Label for the x-axis.\n",
    "        ylabel (str): Label for the y-axis.\n",
    "        log_scale (bool, optional): Whether to use a logarithmic scale for the y-axis. Default is False.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    \n",
    "    if isinstance(peak_counts, list):  # Multiscale case\n",
    "        for scale, peak_count in enumerate(peak_counts):\n",
    "            plt.plot(kappa_th_center, peak_count, label=f'Scale {scale + 1}')\n",
    "    else:  # Single-scale case\n",
    "        plt.plot(kappa_th_center, peak_counts)\n",
    "    \n",
    "    plt.legend()\n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_l1norm_histograms(bins, l1norm_histogram, title, xlabel, ylabel, xlim=None, log_scale=False):\n",
    "    \"\"\"\n",
    "    Plot L1-norm histograms for each scale.\n",
    "\n",
    "    Parameters:\n",
    "        bins (list of numpy.ndarray): List of bin edges for each scale.\n",
    "        l1norm_histogram (list of numpy.ndarray): List of L1-norm histograms for each scale.\n",
    "        title (str): Title of the plot.\n",
    "        xlabel (str): Label for the x-axis.\n",
    "        ylabel (str): Label for the y-axis.\n",
    "        xlim (tuple or list): Limits for the x-axis (e.g., xlim=(0, 6)).\n",
    "        log_scale (bool): Whether to use a logarithmic scale for the y-axis.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "\n",
    "    for scale, l1norm_hist in enumerate(l1norm_histogram):\n",
    "        plt.plot(bins[scale], l1norm_hist, label=f'Scale {scale}')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xticks()\n",
    "    plt.yticks()\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.title(title)\n",
    "    \n",
    "    if xlim:\n",
    "        plt.xlim(xlim)\n",
    "        \n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the catalog data as a numpy array\n",
    "catalog_data = np.loadtxt(CATALOG_FILE)\n",
    "catalog_data = catalog_data.T\n",
    "ra, dec, g1_sim, g2_sim, kappa_sim = read_catalog_data(catalog_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create kappa map and make an SNR map from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kappa map\n",
    "kappaE, _ = create_kappa_map(ra, dec, g1_sim, g2_sim)\n",
    "\n",
    "# Add noise to the kappa map\n",
    "kappaE_noisy, noise_map_CFIS_z05 = add_noise_to_kappa_map(kappaE, SHAPE_NOISE, N_GAL, PIX_ARCMIN)\n",
    "\n",
    "# Smooth the noisy kappa map\n",
    "kappaE_noisy_smoothed = smooth_kappa_map(kappaE_noisy, PIX_ARCMIN) \n",
    "\n",
    "# Compute SNR map\n",
    "snr = convert_to_snr_map(kappaE_noisy_smoothed, kappaE_noisy_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peak counts on the SNR map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute peak counts \n",
    "kappa_th_center_snr, peak_counts_single = compute_single_scale_peak_counts(snr, KAPPA_SNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiscale SNR maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute multiscale SNR maps\n",
    "multiscale_snr_maps = compute_multiscale_snr_maps(kappaE, noise_map_CFIS_z05, NSCALES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute peak counts for each scale\n",
    "kappa_th_center_snr, peak_counts_multi = compute_multiscale_peak_counts(multiscale_snr_maps, KAPPA_SNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ell_1$-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to compute the L1-norm histogram\n",
    "bins_l1, l1norm_histogram = get_l1norm_noisy(kappaE, noise_map_CFIS_z05, NSCALES, NBINS*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot:\n",
    "1. Noiseless mass map\n",
    "2. Noisy mass map\n",
    "3. Noisy smoothed mass map\n",
    "4. SNR map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the map data and titles\n",
    "map_data = [kappaE, kappaE_noisy, kappaE_noisy_smoothed, snr]\n",
    "titles = ['Noiseless Mass Map', 'Noisy Mass Map', 'Noisy Smoothed Mass Map', 'SNR Map']\n",
    "\n",
    "# Define the vmin and vmax values for each map (adjust these values as needed)\n",
    "vmin_values = [-0.01, -0.1, -0.1, None]\n",
    "vmax_values = [0.01, 0.1, 0.1, None]\n",
    "\n",
    "# Plot each map using the plot_map function\n",
    "for data, title, vmin, vmax in zip(map_data, titles, vmin_values, vmax_values):\n",
    "    plot_map(data, title=title, vmin=vmin, vmax=vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the SNR maps of the 5 different scales and the coarse map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vmin and vmax values for each SNR map\n",
    "vmin_values = [-0.5, -3, -4, -4, -4, -4]  # Replace with your desired vmin values\n",
    "vmax_values = [0.5, 3, 4, 4, 4, 4]  # Replace with your desired vmax values\n",
    "\n",
    "scales_arcmin = [2**(i+1) * PIX_ARCMIN for i in range(len(multiscale_snr_maps))]\n",
    "\n",
    "for i, (snr_map, scale_arcmin) in enumerate(zip(multiscale_snr_maps, scales_arcmin)):\n",
    "    title = f\"SNR Map (Scale {scale_arcmin:.1f} arcmin)\"\n",
    "    plot_map(snr_map, title=title, cmap='inferno', vmin=vmin_values[i], vmax=vmax_values[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot single-scale peak count histogram\n",
    "plot_peak_count_histograms(kappa_th_center_snr, peak_counts_single, \n",
    "                           'Peak Counts Histogram for Gaussian SNR Map', 'SNR smooth', 'Peak Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multiscale peak count histograms\n",
    "plot_peak_count_histograms(kappa_th_center_snr, peak_counts, \n",
    "                           'Peak Counts for Different Scales', 'SNR smooth', 'Peak Counts', log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot l1-norm histograms for different scales\n",
    "plot_l1norm_histograms(bins_l1, l1norm_histogram, 'L1-norm Histograms for Different Scales', 'L1-norm', 'Frequency')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
